{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "annees = ['2023'] #'2019', '2020', '2021', '2022', '2023'\n",
    "type_competition = ['Cross'] # Salle', 'Hors+Stade', 'Cross', 'Stade'\n",
    "niveaux = ['D%C3%A9partemental','R%C3%A9gional', 'Interr%C3%A9gional', 'National', 'International', 'Europ%C3%A9en', 'Mondial'] #\n",
    "\n",
    "# %C3%A9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des liens des compétitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_total=[]\n",
    "\n",
    "# On veut les résultats de toutes les compétitions, de tous niveaux et pour chaque année étudiée\n",
    "# Dans un premier temps on récupère tous les liens de ces compétitions\n",
    "\n",
    "for annee1 in annees:\n",
    "\n",
    "    for compet in type_competition: \n",
    "        \n",
    "        \n",
    "        for niveau in niveaux: \n",
    "            \n",
    "            url1=\"https://bases.athle.fr/asp.net/liste.aspx?frmpostback=true&frmbase=resultats&frmmode=2&frmespace=0&frmsaison=\" + str(annee1) + \"&frmtype1=\" + str(compet) + \"&frmtype2=&frmtype3=&frmtype4=&frmniveau=\" + str(niveau) + \"&frmniveaulab=&frmligue=&frmdepartement=&frmeprrch=&frmclub=&frmdate_j1=&frmdate_m1=&frmdate_a1=&frmdate_j2=&frmdate_m2=&frmdate_a2=\"\n",
    "\n",
    "            try: \n",
    "                \n",
    "                soup1= BeautifulSoup(urllib.request.urlopen(url1, timeout=15), 'html.parser')\n",
    "\n",
    "                count = len(soup1.find_all('select', class_='barSelect'))\n",
    "                pages=[]\n",
    "                    \n",
    "                if count > 0: \n",
    "                    options2 = soup1.find_all('select', class_='barSelect')[0].find_all('option')\n",
    "                    pages = [option2['value'] for option2 in options2 if option2['value'] != '']\n",
    "\n",
    "                else: \n",
    "                    pages.append('')\n",
    "                        \n",
    "                # on cherche dans chaque page de résultat pour chaque type de compétition pour une année donnée\n",
    "                        \n",
    "                for p in pages: \n",
    "                            \n",
    "                    url2 = url1 + '&frmposition=' + str(p)\n",
    "                    time.sleep(0.5)\n",
    "                    soup2 = BeautifulSoup(urllib.request.urlopen(url2, timeout=15), 'html.parser')\n",
    "\n",
    "                    links=[]\n",
    "\n",
    "                    for link in soup2.findAll('a', {'class':''})[1::2]:\n",
    "                        link = link.get('href')\n",
    "                        link = 'https://bases.athle.fr'+str(link)\n",
    "                        if \"podiums\" not in link:\n",
    "                            links.append(str(link))\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    links_total = links_total + links\n",
    "                        \n",
    "                    sleep_time = random.uniform(2,4)\n",
    "                    time.sleep(sleep_time)  \n",
    "            \n",
    "            except: \n",
    "                continue  \n",
    "                        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_total = pd.read_excel(\"C:/Users/alexa/OneDrive/Documents/Projets/Données athlé/Donnees/Liens compétitions/liens_cross_2020.xlsx\", index_col=0)\n",
    "links_total = links_total.iloc[:,0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des infos sur la compétition et les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = pd.DataFrame()\n",
    "        \n",
    "# Pour chaque url de compétition ainsi récupéré            \n",
    "\n",
    "for l in links_total: \n",
    "    \n",
    "    url3 = l\n",
    "    \n",
    "    #sleep_time = random.uniform(2,4)\n",
    "    #time.sleep(sleep_time) \n",
    "    \n",
    "    try:\n",
    "        \n",
    "    # on l'ouvre\n",
    "        soup3 = BeautifulSoup(urllib.request.urlopen(url3, timeout=15), 'html.parser')\n",
    "        \n",
    "        # On récupère les infos sur la compétition\n",
    "        \n",
    "        # Extraction de la date et du nom de la compétition\n",
    "\n",
    "        first_part = soup3.find('div', class_='mainheaders').text.split('<br/>')[0].strip()\n",
    "        date, nom_competition_part = first_part.split(' - ', 1)\n",
    "        nom_competition = nom_competition_part.split(' - ')[0].strip()\n",
    "\n",
    "    \n",
    "    # Extraction du lieu, de la region et du département\n",
    "        span_text = soup3.find('div', class_='mainheaders').find('span').text.split(' - ')\n",
    "\n",
    "        if len(span_text)==1: \n",
    "            span_text.append(\"Region etrangere\")\n",
    "            span_text.append(\"Departement etranger\")\n",
    "        if len(span_text) < 4: \n",
    "            span_text.append(\"Pas de label\")\n",
    "        \n",
    "        lieu_competition, region_competition, departement_competition, label_competition = map(str.strip, span_text)\n",
    "\n",
    "        nom_competition = nom_competition.replace(lieu_competition,'').strip()\n",
    "        \n",
    "        if \"kid\" not in nom_competition.lower() and \"lancer\" not in nom_competition.lower() and \"animation\" not in nom_competition.lower() and \"disque\" not in nom_competition.lower() and \"combinées\" not in nom_competition.lower():\n",
    "        \n",
    "            # on récupère les différentes épreuves qui composent la compétition\n",
    "            \n",
    "            epreuves = []\n",
    "            \n",
    "            count2 = len(soup3.find_all('select', class_='barSelect'))\n",
    "            options = soup3.find_all('select', class_='barSelect')[int(count2/2-1)].find_all('option')\n",
    "            \n",
    "            epreuves = [option['value'] for option in options if option['value'] != '']\n",
    "            \n",
    "            # pour chacune de ces épreuves\n",
    "            \n",
    "            \n",
    "            for epreuve in epreuves: \n",
    "                \n",
    "                \n",
    "                # on définit l'url qui lui est associé\n",
    "                url4 = url3 + '&frmepreuve=' + str(epreuve)\n",
    "                \n",
    "            \n",
    "                \n",
    "                #time.sleep(0.5) \n",
    "            \n",
    "                # on l'ouvre\n",
    "                soup4 = BeautifulSoup(urllib.request.urlopen(url4, timeout=15), 'html.parser')\n",
    "                \n",
    "                # on récupère les infos sur l'épreuve\n",
    "                \n",
    "                epreuve = soup4.find('div', class_='subheaders').text.split(' | ')[0]\n",
    "                \n",
    "                if len(soup4.find('div', class_='subheaders').text.split(' | ')) > 4:\n",
    "                    heure_epreuve = re.sub(r'[^0-9:]', '', soup4.find('div', class_='subheaders').text.split(' | ')[-1])\n",
    "                \n",
    "                else: \n",
    "                    heure_epreuve = \"Pas d'horaire\"\n",
    "                \n",
    "                \n",
    "                # on récupère le nombre de pages de résultat de l'épreuve\n",
    "                \n",
    "                count3 = len(soup4.find_all('select', class_='barSelect'))\n",
    "                pages=[]\n",
    "                \n",
    "                if count3 > 6: \n",
    "                    options3 = soup4.find_all('select', class_='barSelect')[0].find_all('option')\n",
    "                    pages = [option3['value'] for option3 in options3 if option3['value'] != '']\n",
    "                else: \n",
    "                    pages.append('')\n",
    "                \n",
    "                # on cherche dans chaque page de résultat pour chaque épreuve\n",
    "                    \n",
    "                for p in pages: \n",
    "                    \n",
    "                    url5 = url4 + '&frmposition=' + str(p)\n",
    "                    \n",
    "                    #time.sleep(0.5) \n",
    "                    \n",
    "                    soup5 = BeautifulSoup(urllib.request.urlopen(url5, timeout=15), 'html.parser')\n",
    "                \n",
    "                    # récupération des infos sur les résultats des athlètes et la compétition sous forme de liste\n",
    "            \n",
    "                    perf = []\n",
    "                    annee_naissance = []\n",
    "                    nom_athlete = []\n",
    "                    categorie = []\n",
    "                    club = []\n",
    "                    ligue = []\n",
    "                    departement = []\n",
    "                    position = []\n",
    "                    \n",
    "                    \n",
    "                    for i in soup5.select('tr td.datas0:first-child, tr td.datas1:first-child'):\n",
    "                        position_i = i.get_text()\n",
    "                        if re.match(\"^[0-9'\\\"]*$\", position_i):\n",
    "                            position.append(str(position_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(3), tr td.datas1:nth-child(3)'):\n",
    "                        perf_i = i.get_text()\n",
    "                        if len(perf_i)<20:\n",
    "                            perf.append(str(perf_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(5), tr td.datas1:nth-child(5)'):\n",
    "                        nom_athlete_i = i.get_text()\n",
    "                        nom_athlete.append(str(nom_athlete_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(7), tr td.datas1:nth-child(7)'):\n",
    "                        club_i = i.get_text()\n",
    "                        club.append(str(club_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(9), tr td.datas1:nth-child(9)'):\n",
    "                        departement_i = i.get_text()\n",
    "                        departement.append(str(departement_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(11), tr td.datas1:nth-child(11)'):\n",
    "                        ligue_i = i.get_text()\n",
    "                        ligue.append(str(ligue_i))\n",
    "                    for i in soup5.select('tr td.datas0:nth-child(13), tr td.datas1:nth-child(13)'):\n",
    "                        categorie_i = i.get_text()\n",
    "                        cat=categorie_i.split('/')[0]\n",
    "                        annee=categorie_i.split('/')[1]\n",
    "                        categorie.append(str(cat))\n",
    "                        annee_naissance.append(str(annee))\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    # transformation des listes en dataframe\n",
    "                    \n",
    "                    donnees = {'Position': position,\n",
    "                                'Nom_athlete': nom_athlete,\n",
    "                                'Performance': perf,\n",
    "                                'Annee_naissance': annee_naissance, \n",
    "                                'Categorie': categorie,\n",
    "                                'Club': club,\n",
    "                                'Département': departement,\n",
    "                                'Ligue': ligue, \n",
    "                                'Epreuve': epreuve,\n",
    "                                'Nom_competition': nom_competition,\n",
    "                                'Lieu_competition': lieu_competition,\n",
    "                                'Date_competition': date,\n",
    "                                'Heure_epreuve': heure_epreuve,\n",
    "                                'Departement_competition': departement_competition,\n",
    "                                'Region_competition': region_competition,\n",
    "                                'Label_competition': label_competition}\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    resultats_partiels = pd.DataFrame(donnees)\n",
    "                    resultats = pd.concat([resultats, resultats_partiels], ignore_index=True)\n",
    "            \n",
    "    except: \n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats.to_excel(f'Resultats_cross_2020.xlsx') #_{compet}_{annee1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test récupération des données pour une compétition donnée\n",
    "# \n",
    "#\n",
    "resultats = pd.DataFrame()\n",
    "\n",
    "url2 = \"https://bases.athle.fr/asp.net/liste.aspx?frmbase=resultats&frmmode=1&frmespace=0&frmcompetition=275068\"\n",
    "    \n",
    "    # on l'ouvre\n",
    "soup2 = BeautifulSoup(urllib.request.urlopen(url2, timeout=15), 'html.parser')\n",
    "    \n",
    "    # On récupère les infos sur la compétition\n",
    "    \n",
    "    # Extraction de la date\n",
    "\n",
    "first_part = soup2.find('div', class_='mainheaders').text.split('<br/>')[0].strip()\n",
    "date, nom_competition_part = first_part.split(' - ', 1)\n",
    "nom_competition = nom_competition_part.split(' - ')[0].strip()\n",
    "\n",
    "\n",
    "    # Extraction du lieu, de la region et du département\n",
    "span_text = soup2.find('div', class_='mainheaders').find('span').text.split('-')\n",
    "\n",
    "if len(span_text)==1: \n",
    "    span_text.append(\"Region etrangere\")\n",
    "    span_text.append(\"Departement etranger\")\n",
    "if len(span_text) < 4: \n",
    "    span_text.append(\"Pas de label\")\n",
    "    \n",
    "lieu_competition, region_competition, departement_competition, label_competition = map(str.strip, span_text)\n",
    "\n",
    "nom_competition = nom_competition.replace(lieu_competition,'').strip()\n",
    "    \n",
    "    # on récupère les différentes épreuves qui composent la compétition\n",
    "    \n",
    "epreuves = []\n",
    "    \n",
    "count = len(soup2.find_all('select', class_='barSelect'))\n",
    "options = soup2.find_all('select', class_='barSelect')[int(count/2-1)].find_all('option')\n",
    "    \n",
    "epreuves = [option['value'] for option in options if option['value'] != '']\n",
    "    \n",
    "for epreuve in epreuves: \n",
    "        \n",
    "        # on définit l'url qui lui est associé\n",
    "    url3 = url2 + '&frmepreuve=' + str(epreuve)        \n",
    "        # on définit l'url qui lui est associé\n",
    "        # on l'ouvre\n",
    "    soup3 = BeautifulSoup(urllib.request.urlopen(url3, timeout=15), 'html.parser')\n",
    "        \n",
    "        # on récupère les infos sur l'épreuve\n",
    "        \n",
    "    epreuve = soup3.find('div', class_='subheaders').text.split(' | ')[0]\n",
    "        \n",
    "    if len(soup3.find('div', class_='subheaders').text.split(' | ')) > 4:\n",
    "        heure_epreuve = re.sub(r'[^0-9:]', '', soup3.find('div', class_='subheaders').text.split(' | ')[-1])\n",
    "        \n",
    "    else: \n",
    "        heure_epreuve = \"Pas d'horaire\"\n",
    "        \n",
    "        \n",
    "        # on récupère le nombre de pages de résultat de l'épreuve\n",
    "        \n",
    "    count2 = len(soup3.find_all('select', class_='barSelect'))\n",
    "    pages=[]\n",
    "        \n",
    "    if count2 > 6: \n",
    "        options2 = soup3.find_all('select', class_='barSelect')[0].find_all('option')\n",
    "        pages = [option2['value'] for option2 in options2 if option2['value'] != '']\n",
    "\n",
    "    else: \n",
    "        pages.append('')\n",
    "        \n",
    "# on cherche dans chaque page de résultat pour chaque épreuve\n",
    "            \n",
    "    for p in pages: \n",
    "            \n",
    "        url4 = url3 + '&frmposition=' + str(p)\n",
    "            \n",
    "        soup4 = BeautifulSoup(urllib.request.urlopen(url4, timeout=15), 'html.parser')\n",
    "        \n",
    "            # récupération des infos sur les résultats des athlètes et la compétition sous forme de liste\n",
    "    \n",
    "        perf = []\n",
    "        annee_naissance = []\n",
    "        nom_athlete = []\n",
    "        categorie = []\n",
    "        club = []\n",
    "        ligue = []\n",
    "        departement = []\n",
    "        position = []\n",
    "            \n",
    "            \n",
    "        for i in soup4.select('tr td.datas0:first-child, tr td.datas1:first-child'):\n",
    "            position_i = i.get_text()\n",
    "            position.append(str(position_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(3), tr td.datas1:nth-child(3)'):\n",
    "            perf_i = i.get_text()\n",
    "            perf.append(str(perf_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(5), tr td.datas1:nth-child(5)'):\n",
    "            nom_athlete_i = i.get_text()\n",
    "            nom_athlete.append(str(nom_athlete_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(7), tr td.datas1:nth-child(7)'):\n",
    "            club_i = i.get_text()\n",
    "            club.append(str(club_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(9), tr td.datas1:nth-child(9)'):\n",
    "            departement_i = i.get_text()\n",
    "            departement.append(str(departement_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(11), tr td.datas1:nth-child(11)'):\n",
    "            ligue_i = i.get_text()\n",
    "            ligue.append(str(ligue_i))\n",
    "        for i in soup4.select('tr td.datas0:nth-child(13), tr td.datas1:nth-child(13)'):\n",
    "            categorie_i = i.get_text()\n",
    "            cat=categorie_i.split('/')[0]\n",
    "            annee=categorie_i.split('/')[1]\n",
    "            categorie.append(str(cat))\n",
    "            annee_naissance.append(str(annee))\n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            # transformation des listes en dataframe\n",
    "            \n",
    "        donnees = {'Position': position,\n",
    "                         'Nom_athlete': nom_athlete,\n",
    "                         'Performance': perf,\n",
    "                         'Annee_naissance': annee_naissance, \n",
    "                         'Categorie': categorie,\n",
    "                         'Club': club,\n",
    "                         'Département': departement,\n",
    "                         'Ligue': ligue, \n",
    "                         'Epreuve': epreuve,\n",
    "                         'Nom_competition': nom_competition,\n",
    "                         'Lieu_competition': lieu_competition,\n",
    "                         'Date_competition': date,\n",
    "                         'Heure_epreuve': heure_epreuve,\n",
    "                         'Departement_competition': departement_competition,\n",
    "                         'Region_competition': region_competition,\n",
    "                         'Label_competition': label_competition\n",
    "                         }\n",
    "    \n",
    "        resultats_partiels = pd.DataFrame(donnees)\n",
    "            \n",
    "        resultats = pd.concat([resultats, resultats_partiels], ignore_index=True)  \n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "https://bases.athle.fr/asp.net/liste.aspx?frmpostback=true&frmbase=resultats&frmmode=2&frmespace=0&frmsaison=2023&frmtype1=Salle&frmtype2=&frmtype3=&frmtype4=&frmniveau=Interr%C3%A9gional&frmniveaulab=&frmligue=&frmdepartement=&frmeprrch=&frmclub=&frmdate_j1=&frmdate_m1=&frmdate_a1=&frmdate_j2=&frmdate_m2=&frmdate_a2=&frmposition=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test prise en compte du numéro de page\n",
    "\n",
    "url='https://bases.athle.fr/asp.net/liste.aspx?frmpostback=true&frmbase=resultats&frmmode=2&frmespace=0&frmsaison=2023&frmtype1=Salle&frmtype2=&frmtype3=&frmtype4=&frmniveau=Interr%C3%A9gional&frmniveaulab=&frmligue=&frmdepartement=&frmeprrch=&frmclub=&frmdate_j1=&frmdate_m1=&frmdate_a1=&frmdate_j2=&frmdate_m2=&frmdate_a2='\n",
    "\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url, timeout=15), 'html.parser')\n",
    "\n",
    "count = len(soup.find_all('select', class_='barSelect'))\n",
    "print(count)\n",
    "\n",
    "pages=[]\n",
    "        \n",
    "if count > 0: \n",
    "     options2 = soup.find_all('select', class_='barSelect')[0].find_all('option')\n",
    "     pages = [option2['value'] for option2 in options2 if option2['value'] != '']\n",
    "\n",
    "else: \n",
    "     pages.append('')\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# Afficher les valeurs extraites\n",
    "\n",
    "\n",
    "print(url + '&frmposition=' + pages[0])\n",
    "\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Riorges ', ' ARA ', ' 042 ', ' Label Régional']\n",
      "05/11/23 - 10 km de RiorgesRiorges - ARA - 042 - Label Régional\n",
      "Date: 05/11/23\n",
      "Nom de la compétition: 10 km de\n",
      "Lieu de la compétition: Riorges\n",
      "Région: ARA\n",
      "Département: 042\n",
      "Label Régional\n",
      "['05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23', '05/11/23']\n"
     ]
    }
   ],
   "source": [
    "## Test récupération infos sur la compétition\n",
    "\n",
    "\n",
    "url = \"https://bases.athle.fr/asp.net/liste.aspx?frmbase=resultats&frmmode=1&frmespace=0&frmcompetition=279052\"\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url, timeout=15), 'html.parser')\n",
    "\n",
    "# Extraction de la date\n",
    "\n",
    "first_part = soup.find('div', class_='mainheaders').text.split('<br/>')[0].strip()\n",
    "date, nom_competition_part = first_part.split(' - ', 1)\n",
    "nom_competition = nom_competition_part.split(' - ')[0].strip()\n",
    "\n",
    "\n",
    "\n",
    "# Extraction du lieu, de la region et du département\n",
    "span_text = soup.find('div', class_='mainheaders').find('span').text.split('-')\n",
    "\n",
    "if len(span_text) < 4: \n",
    "    span_text.append(\"Pas de label\")\n",
    "    \n",
    "lieu_competition, region_competition, departement_competition, label_competition = map(str.strip, span_text)\n",
    "\n",
    "nom_competition = nom_competition.replace(lieu_competition,'').strip()\n",
    "\n",
    "\n",
    "print(span_text)\n",
    "print(first_part)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f'Date: {date}')\n",
    "print(f'Nom de la compétition: {nom_competition}')\n",
    "print(f'Lieu de la compétition: {lieu_competition}')\n",
    "print(f'Région: {region_competition}')\n",
    "print(f'Département: {departement_competition}')\n",
    "print(label_competition)\n",
    "\n",
    "date = [date] * 10\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U16 masculin MIM\n",
      "4510 m\n",
      "11:05\n"
     ]
    }
   ],
   "source": [
    "## Test récuération infos sur l'épreuve\n",
    "\n",
    "url = \"https://bases.athle.fr/asp.net/liste.aspx?frmbase=resultats&frmmode=1&frmespace=0&frmcompetition=275068&frmepreuve=U16+masculin+MIM\"\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url, timeout=15), 'html.parser')\n",
    "\n",
    "epreuve = soup.find('div', class_='subheaders').text.split(' | ')[2]\n",
    "print(epreuve)\n",
    "distance = soup.find('div', class_='subheaders').text.split(' | ')[4]\n",
    "print(distance)\n",
    "heure_competition = re.sub(r'[^0-9:]', '', soup.find('div', class_='subheaders').text.split(' | ')[6])\n",
    "print(heure_competition)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
